{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6fadb2a",
   "metadata": {},
   "source": [
    "# DeepSeek LLM Deployment on SageMaker\n",
    "This notebook demonstrates how to deploy the DeepSeek-R1-Distill-Llama-8B model on Amazon SageMaker using the Hugging Face Deep Learning Container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a5e34-f01c-4c76-a1d7-aff4ce07b2ee",
   "metadata": {},
   "source": [
    "# Install the latest version of SageMaker SDK\n",
    "This helps you ensure compatibility with huggingface-llm library used in hosting models from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6701c5-bc51-40a6-8dec-fe768d5cd69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83c387",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "Define the configuration for the DeepSeek model:\n",
    "- `HF_MODEL_ID`: Specifies the Hugging Face model to be deployed\n",
    "- `SM_NUM_GPUS`: Sets the number of GPUs to use for the deployment (1 in this case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e728ea8a-634e-4cf0-8fca-c35e81aee790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
    "\t'SM_NUM_GPUS': json.dumps(1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41fe0d",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "This cell deploys the model to a SageMaker endpoint with the following configuration:\n",
    "- Uses a g5.2xlarge instance type for GPU acceleration\n",
    "- Deploys in a VPC with specified subnets and security groups\n",
    "- Sets an extended health check timeout for model loading\n",
    "\n",
    "**Note:** The VPC argument details are optional, but highly recommended as they help you be in control of security. \n",
    "\n",
    "KMS key for encrypting EBS volume used by the hosting instance is not needed in this case since the instance store volume will be used. Relevant snippet from the documentation:\n",
    "\n",
    ">The data on NVMe instance store volumes is encrypted using an XTS-AES-256 cipher, implemented on a hardware module on the instance. The keys used to encrypt data that's written to locally-attached NVMe storage devices are per-customer, and per volume. The keys are generated by, and only reside within, the hardware module, which is inaccessible to AWS personnel. The encryption keys are destroyed when the instance is stopped or terminated and cannot be recovered. You cannot disable this encryption and you cannot provide your own encryption key.\n",
    "Reference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b94d17-6077-4c9a-9c4b-0f666ce7f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"2.3.1\"),\n",
    "\tenv=hub,\n",
    "\trole=role,\n",
    " # Following is optional, but highly recommended\n",
    "    vpc_config={\n",
    "        'Subnets': [\n",
    "            '<ENTER YOUR SUBNET 1 ID>',\n",
    "            '<ENTER YOUR SUBNET 1 ID>'\n",
    "        ],\n",
    "        'SecurityGroupIds': [\n",
    "            '<ENTER YOUR SECURITY GROUP ID>'\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=900,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294b5ae",
   "metadata": {},
   "source": [
    "## Making Predictions with the Deployed Model\n",
    "The following cell demonstrates how to send a request to the deployed model:\n",
    "- We use the `predict()` method of our endpoint predictor\n",
    "- The request includes:\n",
    "  - `inputs`: The text prompt we want the model to process\n",
    "  - `parameters`: Configuration for the generation\n",
    "    - `max_length`: Maximum length of the entire sequence (input + generated text)\n",
    "    - `max_new_tokens`: Maximum number of tokens to generate\n",
    "\n",
    "The model will return a response comparing the two numbers provided in the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53415d40-264b-4fd1-8120-151e73723b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Which is larger 9.11 or 9.8?\",\n",
    "    \"parameters\": {\n",
    "        \"max_length\": 4096,\n",
    "        \"max_new_tokens\": 2048\n",
    "    }\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
